

# Principal Component Analysis and Clustering

## Assignment 4 - Gentle Intro to Machine Learning

### Preparing the Data for Principle Component Analysis

Using the TEDS2016 dataset, we explore groupings of the data through unsupervised learning. By this, we are refering to a kind of initial discovery phase without a specified hypothesis or output/inputs. We focus on two common methods of machine learning: Principal Component Analysis (PCA) and clustering. Remember the TEDS2016 data has 54 variables and 1690 observations. Starting with PCA, this method is most useful when there are numerous continuous metrics to sift through and we want to identify just the top most influential with the least collinearity. PCA is known for identifying or improving dimensional quality to model.

First, let's remove the variables that cannot be used in this technique. Below we remove the elements with a variance of 0 since it cannot be rescaled for the purposes of this method. We also remove all cases with missing values using the dplyr package, leaving us with 1074 cases.

```{r}
library(dplyr)
library(haven)
TEDS_2016 <- read_stata("https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true")
TEDSvar <- apply(TEDS_2016, 2, var)
print(TEDSvar)

library(dplyr)
PCATEDS <- TEDS_2016 %>% select_if(function(x) var(x, na.rm = T) !=0)
PCATEDS <- PCATEDS[complete.cases(PCATEDS),]
```

Now the data is ready for PCA

```{r}
PCA <- prcomp(PCATEDS, scale = T)
summary(PCA)
```

The results show 51 significant variance proportions. This is somewhat more specified than the 54 variable data set. The weighted sum of PC1 explains .23 of the variance in 54 variables. At PC30, we reach .99 cumulative proportion, drastically clarifying potential redundancies from the data set. This is also visualized below.

```{r}
library(ggplot2)
ggplot(data.frame(PC = 1:min(length(PCA$sdev), 30), Variance = (PCA$sdev[1:min(length(PCA$sdev), 30)]^2 / sum(PCA$sdev^2))), aes(x = PC, y = Variance))+
geom_point(color = "red", size = 3) +
geom_line(size = 1.2) +
scale_x_continuous(breaks = 1:min(length(PCA$sdev), 30)) +
labs(x = "Principal Component", y = "Variance Explained", title = "Scree Plot")+
theme(panel.grid.minor = element_blank(),
      panel.grid.major.x = element_line(color = "gray85"),
      panel.grid.major.y = element_line(color = "gray85"),
      panel.border = element_blank(),
      axis.line = element_line(color = "gray50"))
```

### Visualizing Principal Components

Visualization formats should be chosen based on what elements of the data that have proven most relevant. Below we show two visualization examples, one macro and one micro. First, we wish to show how variables from the original data can be projected onto components discovered. We can use this visualization to potentially identify new potential variables and vet existing ones. Below we chose to project the variable Tondu onto PC1 and PC2 , which will represent the axes. Remember, Tondu is a factor with 7 level survey responses providing attitudes on towards Taiwan's potential to reunify with China. By, projecting the original data onto the new eigenvector data, to see if we can discover a sense of what PC1 and PC2 could be correlated with in Tondu.

```{r}
vizPCA <- as.data.frame(PCA$x) #Transformed Eigenvalues 
vizPCA$Tondu <- PCATEDS$Tondu #Add Parent Data for Tondu
vizPCA$Tondu <- factor(vizPCA$Tondu, levels=c('1','2','3', '4', '5','6', '9'),
labels=c('Unification now','Status quo, unif. in future','Status quo, decide later',
'Status quo forever', 'Status quo, indep. in future', 'Independence now',
'No response'))
library(ggplot2)
ggplot(vizPCA) + geom_point(aes(x=vizPCA[,1], y=vizPCA[,2], color=Tondu))
```

As you can see, there are some clear and some vague groupings here. Two major groupings stand out and depict a bi-modal distribution. Interestingly, you can see how some special combination of PC1 and PC2 at -5 standard deviations contains one of two groupings. This group contains most of the responses in support of Status Quo, Unification in Future while the other grouping at standard deviation 2.5 contains most of the responses for Status Quo, Independence in Future. We can deduce that PC1 and PC2 offer some insight that may significantly contribute to how Taiwanese voters find themselves in one of these two schools of thought. These two major groupings are relatively distinct in the graph and supports the theory that these two stances are diametrically opposed. The same can be said for Unification Now and Independence Now which are both exponentially smaller in frequency than their previously mentioned counterparts. Status Quo, Decide Later is distributed throughout both groups and take up the most space in between groups which indicates it's versatility as a preference. Perhaps, Status Quo, Decide Later is less useful in making distinctions than others will be.

What is most interesting is Status Quo, Forever. The frequency of Status Quo Forever is more closely tied with varying stances in support of Unification (-5 SD grouping), while its opposing group has significantly fewer instances. This is our clue that perhaps projecting Chinese or Taiwanese heritage onto PCA data might reveal or confirm correlations for weaker/stronger ties with China. It behooves us to try a few different visualizations. Some like to start with a macro visualization with as many inputs as possible, others like to start with micro visualizations. It just depends on how your own mental faculties are best assisted. Below, we use the  not go into macro visualization where all original variables are projected visually with all principal components as recoding the inputs into factor levels would be too tedious here. We will leave it for another day!
