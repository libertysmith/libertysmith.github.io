[
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "libertysmith.github.io",
    "section": "",
    "text": "Hypothesis: Are high rates of screen time on a hand-held device during elementary school years associated with a high probability of ADHD-like symptoms?\n\n\n\nIn recent years, a high rate of interest and concern over attention deficits surrounding device usage has emerged. The concern stems from the idea that increased incidences of ADHD-like symptoms such as lack of patience, impulse control, and focus are due to rising screen time experienced as adolescents. This study is NOT a rigid study on the probabilities of an individual having a diagnoses of ADHD (although we think similar directionalities will be found). This study has a flexible definition to mean ADHD-like symptoms, so as to also include people exhibiting symptoms very similar to ADHD, but perhaps is either undiagnosed or falls out of the medical definition of ADHD. The best method, based on previous experience, is to do a logistic regression, classifying people into two classes for the output: 1) high-instances of ADHD symptoms and 2) low-instances of ADHD symptoms. My methods are typically data modeling based because of my recent experience.  \n\n\nThe data to be sourced should be in service of a uniform quantitative metrics or scale assessing the extent of the participants’ attention deficits. We can accomplish this with survey data about each observation’s attention deficit prior to the study, then again 10 years later after the study. The inputs are total weekly screen time scraped from historical Apple data during the ages between 5 and 11 and additionally the weekly amount of screen time devoted to certain activities such as social media, texting, streaming, video games, etc. \n\n\nBased on my personal experience so far with parametric modeling procedures, I would pursue logistic regression with theoretical implications. In doing so, I anticipate higher probabilities associated with participants using their devices for more kinds of activities as well as those with higher total averages than their counterparts. Participants who already score high on ADHD like symptoms cannot be considered for participation. One or more  samples ending with a minimum of 1500 elementary aged American children would be sufficient for our purposes. \n\n\nDuring exploratory data analysis, data visualization (univariate, bivariate) relationships will be examined and transformed as needed. We will need to examine various tests on residuals and interaction terms to test nested models and model validity. \n\n\nTo start the project, we must approach the proper entities of authority to complete due diligence with ethical procedures surrounding human subjects. We must find funding for the project from the most relevant stakeholders."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Curriculumvitae.html",
    "href": "Curriculumvitae.html",
    "title": "libertysmith.github.io",
    "section": "",
    "text": "LinkedIn Page\n\n\nThe University of Texas at Dallas, (M.S., Social Data Analytics and Research) May 2024\nThe University of Texas at Dallas, (Bachelor of Arts in Political Science) May 2017\n\n\n\nKorea Economic Institute of America, Washington, D.C., USA (Economic Intern) Sept 2018 - Jan 2019\n* Led and assisted in statistical research and writing published on “The Peninsula” – think tank sponsored blog – alongside legacy media journalists and diplomats. * Collaborated with stakeholders, researched, gauged value of Korea-US foreign trade and social policy. Suggested policy development directions for greater equitable economic policies and criminology.\n* Wrote and edited social media content for organizational content promotion on Twitter and Facebook. Extended reach of viewership by 15%, engagement analysis utilizing SproutSocial and MailChimp metrics.\nSuitsupply, Dallas, Texas, USA (Web Sales and Customer Service Analyst) Dec 2021 - Present\n* Promoted the adoption of new Salesforce practices and specialized features increasing productivity and reducing team burnout and turnover by 50%, leading to relative increase of 5-15% in gross monthly sales.\n* Facilitated trainings on retail policy implementation to improve the tracking, execution, reporting, release and management of Product Knowledge.\n* Optimized the critical and recurrent concerns by analyzing Salesforce CRMS client feedback, improved the overall consistency and user experience of customer service, significantly increasing the ratings from 3.8 to 4.2, thus reducing direct contact ratio from 15% to 9% in six months.\n* Ensured complete user journeys with relevant error handling and complete feedback loops considering UX principles, user behavior & tendencies improving the overall consistency and experience of the customer service lifecycle.\n* Earned the rating of ‘Exceptional’ for 2 consecutive years and recognized among the top 3 performers in the branch office.\n\n\n\nActs of Kindness at UTD – *Founder and Treasurer Fall 2014 - Spring 2016\n* Leading a team of 12 members, proposed budget, organized the Council events, led presentation of group selected topics to advocate awareness on campus, coordinated Acts of Kindness drives (services) that supported students struggling financially during exam weeks.\n* Promoted inter and intra-college participation by strategizing existing modes of communication, created online presence, and multiplied reach utilizing social media.\n\n\n\nTechnical Skills: MySQL, Python, R, HTML, CSS, React, Google Analytics, Adobe Analytics, SalesForce Admin, MS Project Professional, MS Office, Project Management, Product Management, ML and Statistical Data Analytics, probability modelling, research and brief writing.\nLanguages: Advanced English and Conversational Korean\nEligibility: U.S. Citizen Authorized to work in the United States"
  },
  {
    "objectID": "litreview.html",
    "href": "litreview.html",
    "title": "libertysmith.github.io",
    "section": "",
    "text": "This is a comparative literature review between Leo Breiman’s paper “Statistical Modeling: The Two Cultures” and Galit Shmueli’s piece “To Explain or to Predict?”\n\n\nIn 2001, Berkeley professor Leo Breiman wrote a seminole paper in which he convincingly advocates for the growth of the algorithm culture over the data modeling culture due to the trade off between interpretability and accuracy. Breiman argues algorithmic culture is a different caliber that demands more focus. Understanding what happens in the black box is marginally less important than the potential of its applied knowledge. Thus, algorithmic processes are less discriminatory and enable better knowledge mining than data modeling. He provides several examples of how its methods often precede improved accuracy, relative to data modeling methods. Data modeling, reasoned Breiman, had become too limited by its underlying assumptions such as its demands for theoretical relationships and interpretability. How to improve efficiency, how to improve greenhouse gas emissions, how to detect extreme weather events, any outcome-oriented goals are - or were - the pressing reasons to re-prioritize the two cultures. \n9 years later, author of “To Explain or to Predict?” by Galit Schmueli brings an updated  perspective to the table. Both authors agree a cultural gap resulted in two distinct modeling rationales which has led to data modeling culture’s narrowing breadth of knowledge extraction. In contrast to Breiman, he does not advocate for one culture’s rigor or survival over the other. By the time of his publication, a significant number of practices shared between the two cultures are identifiable. Thus, the two are not as rigidly defined. For example, traces of model validity and use commonly interchangeable tools such as partitioning and visualization. However, the tools are worked in different fashions, as led by the a priori project needs. Though he agrees a considerable gap remains (in four aspects) between the two cultures, algorithm modeling has caught up or even supersedes data modeling’s practicality in many regards. \n\nIn fact, Schmeuli argues the bridge to divide the knowledge gap should go both ways. The use of algorithmic modeling techniques have earned their place in research science and should not be limited to the specific spaces already associated with it. The ultimate pursuit of science demands so. The innovative methods and growth of algorithmic culture improve upon data modeling practices and the reverse is also true. The underlying wisdom of data modeling has more knowledge to impart to the bleeding edge of algorithmic culture.  \n\nAll in all, the two papers are emblematic of the state of affairs in their respective eras. The evolution of data science is a story about humans urgently adapting through iterative sciences with the singular goal of gaining useful knowledge. How “useful knowledge” is defined seems to largely hinge on how one values the scale between prediction and interpretability. \n1/31/2023 Libby Smith"
  },
  {
    "objectID": "Lab1.html",
    "href": "Lab1.html",
    "title": "EPPS 6302: Lab01",
    "section": "",
    "text": "x <- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=TRUE) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9942377\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y, pch=20, col = \"blue\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"blue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "Lab2.html",
    "href": "Lab2.html",
    "title": "EPPS 6323: Lab02",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\")\n# fix(Auto) # Starting the X11 R data editor\nAuto=read.table(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.data\",header=T,na.strings=\"?\")\n# fix(Auto)\nAuto=read.csv(\"https://raw.githubusercontent.com/karlho/knowledgemining/gh-pages/data/Auto.csv\",header=T,na.strings=\"?\")\n# fix(Auto)\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,]\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\npackage 'MASS' successfully unpacked and MD5 sums checked\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\LSmith\\AppData\\Local\\Temp\\RtmpEjzQ1X\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.2.2\n\n\nLoading required package: ISLR\n\n\nWarning: package 'ISLR' was built under R version 4.2.2\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.55384    0.56263   61.41   <2e-16 ***\nlstat       -0.95005    0.03873  -24.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  < 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Libby Smith: The Holistic Data Scientist",
    "section": "",
    "text": "Libby is a budding data scientist with a political science undergraduate degree. She is completing a Master of Science in Social Data Analytics and Research with a tentative graduation date of May 2024. She aims to be a well-rounded candidate for private and public sector organizations with a mission to grow while in pursuit of results-oriented knowledge. She has experience in foreign trade policy, litigation, real estate, and online retail both at granular and systemic levels."
  },
  {
    "objectID": "index.html#more-information",
    "href": "index.html#more-information",
    "title": "Libby Smith: The Holistic Data Scientist",
    "section": "More Information",
    "text": "More Information\nInterests:\n* Political Economy\n* Logistic Regression & Probability Modeling\n* Machine Learning Modeling\n\n\nContact Me\nlxs121330@utdallas.edu\nLinkedIn"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "Using the TEDS2016 dataset, I explore different issues that arise from analysis of 1690 observations of 54 variables. Using this data, I generate graphics and computations to describe key relationships. TEDS is an acronym for Taiwan’s Election and Democratization Study, 2012-2016 (IV): the Survey of the Presidential and Legislative Elections.\n\nlibrary(haven)\n#We are working with the haven package.\n#If not already, the package will need to be installed prior to this command. \n\nTEDS_2016 <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\n\nVotesai, female, DPP, age, income, edu, Taiwanese, and Econ_worse.\nWhen encountering missing values, I put my social sciences hat on and determine that missing values should not simply be removed. In any professional setting in pursuit of social data, the external and internal validation of the potential models can be at risk if missing values are not properly computed. The reason for the data being missing should be determined and responded to. The response should attempt to resolve the potential bias in the data by retrieving the missing value(s) and the reason for its missingness, should the budget allow. Should the project be hindered by budget or logistical constraints, it is best to move forward with replacing missing values with the vector’s average(s).\n\nNA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE)) #Saving the funciton to Environment\n\nreplace(TEDS_2016, TRUE, lapply(TEDS_2016, NA2mean))\n\n# A tibble: 1,690 × 54\n   District     Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n   <dbl+lbl>    <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+l> <dbl+lb>\n 1 201 [Yi Lan… 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n 2 201 [Yi Lan… 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n 3 201 [Yi Lan… 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n 4 201 [Yi Lan… 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n 5 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n 6 201 [Yi Lan… 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n 7 201 [Yi Lan… 1 [Mal… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 2 [Bot… 25 [Neu…\n 8 201 [Yi Lan… 2 [Fem… 4 [50-… 5 [Abo… 1 [Tai… 1 [Hig… 2 [Man… 1 [Tai… 24 [Som…\n 9 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n10 201 [Yi Lan… 1 [Mal… 4 [50-… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 2 [Bot… 25 [Neu…\n# … with 1,680 more rows, and 45 more variables: PartyID <dbl+lbl>,\n#   Tondu <dbl+lbl>, Tondu3 <dbl+lbl>, nI2 <dbl+lbl>, votetsai <dbl>,\n#   green <dbl>, votetsai_nm <dbl>, votetsai_all <dbl>, Independence <dbl>,\n#   Unification <dbl>, sq <dbl>, Taiwanese <dbl>, edu <dbl>, female <dbl>,\n#   whitecollar <dbl>, lowincome <dbl>, income <dbl>, income_nm <dbl>,\n#   age <dbl>, KMT <dbl>, DPP <dbl>, npp <dbl>, noparty <dbl>, pfp <dbl>,\n#   South <dbl>, north <dbl>, Minnan_father <dbl>, Mainland_father <dbl>, …\n\n\n\n\n\nFor the first half of exploratory search, I use statistical modeling methods to evaluate uni variate and bivariate relationships. The benefit of this method is to examine several pairwise and lone variables simultaneously to test several theoretical parameters. The downside of this method is that undefined metrics of unknown parameters potentially influencing the hypothesis of interest here cannot be identified and explored this way.\nOne of my favorite ways to generate reports on data structure for initial analysis is with the packages dplyr and DataExplorer. This method generates a number of key graphs and figures providing a basis for which relationships to investigate further.\n\ninstall.packages=\"dplyr\"\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nChosenRows <- TEDS_2016 %>% select(female, DPP, age, income, edu, Taiwanese, Tondu, Econ_worse, votetsai) #Create dataframe with irrelevant variables removed.\n\ninstall.packages=\"DataExplorer\"\nlibrary(DataExplorer)\n\nWarning: package 'DataExplorer' was built under R version 4.2.2\n\ncreate_report(ChosenRows) #Generate Rmarkdown report (Seen Below)\n\n\n\nprocessing file: report.rmd\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |..                                                                    |   2%\n   inline R code fragments\n\n\n  |                                                                            \n  |...                                                                   |   5%\nlabel: global_options (with options) \nList of 1\n $ include: logi FALSE\n\n\n  |                                                                            \n  |.....                                                                 |   7%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.......                                                               |  10%\nlabel: introduce\n\n  |                                                                            \n  |........                                                              |  12%\n  ordinary text without R code\n\n\n  |                                                                            \n  |..........                                                            |  14%\nlabel: plot_intro\n\n\n\n  |                                                                            \n  |............                                                          |  17%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.............                                                         |  19%\nlabel: data_structure\n\n  |                                                                            \n  |...............                                                       |  21%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.................                                                     |  24%\nlabel: missing_profile\n\n\n\n  |                                                                            \n  |..................                                                    |  26%\n  ordinary text without R code\n\n\n  |                                                                            \n  |....................                                                  |  29%\nlabel: univariate_distribution_header\n\n  |                                                                            \n  |......................                                                |  31%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.......................                                               |  33%\nlabel: plot_histogram\n\n\n\n  |                                                                            \n  |.........................                                             |  36%\n  ordinary text without R code\n\n\n  |                                                                            \n  |...........................                                           |  38%\nlabel: plot_density\n\n  |                                                                            \n  |............................                                          |  40%\n  ordinary text without R code\n\n\n  |                                                                            \n  |..............................                                        |  43%\nlabel: plot_frequency_bar\n\n  |                                                                            \n  |................................                                      |  45%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.................................                                     |  48%\nlabel: plot_response_bar\n\n  |                                                                            \n  |...................................                                   |  50%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.....................................                                 |  52%\nlabel: plot_with_bar\n\n  |                                                                            \n  |......................................                                |  55%\n  ordinary text without R code\n\n\n  |                                                                            \n  |........................................                              |  57%\nlabel: plot_normal_qq\n\n\n\n  |                                                                            \n  |..........................................                            |  60%\n  ordinary text without R code\n\n\n  |                                                                            \n  |...........................................                           |  62%\nlabel: plot_response_qq\n\n  |                                                                            \n  |.............................................                         |  64%\n  ordinary text without R code\n\n\n  |                                                                            \n  |...............................................                       |  67%\nlabel: plot_by_qq\n\n  |                                                                            \n  |................................................                      |  69%\n  ordinary text without R code\n\n\n  |                                                                            \n  |..................................................                    |  71%\nlabel: correlation_analysis\n\n\n\n  |                                                                            \n  |....................................................                  |  74%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.....................................................                 |  76%\nlabel: principal_component_analysis\n\n\n\n  |                                                                            \n  |.......................................................               |  79%\n  ordinary text without R code\n\n\n  |                                                                            \n  |.........................................................             |  81%\nlabel: bivariate_distribution_header\n\n  |                                                                            \n  |..........................................................            |  83%\n  ordinary text without R code\n\n\n  |                                                                            \n  |............................................................          |  86%\nlabel: plot_response_boxplot\n\n  |                                                                            \n  |..............................................................        |  88%\n  ordinary text without R code\n\n\n  |                                                                            \n  |...............................................................       |  90%\nlabel: plot_by_boxplot\n\n  |                                                                            \n  |.................................................................     |  93%\n  ordinary text without R code\n\n\n  |                                                                            \n  |...................................................................   |  95%\nlabel: plot_response_scatterplot\n\n  |                                                                            \n  |....................................................................  |  98%\n  ordinary text without R code\n\n\n  |                                                                            \n  |......................................................................| 100%\nlabel: plot_by_scatterplot\n\n\noutput file: C:/Users/LSmith/Documents/Github/report.knit.md\n\n\n\"C:/Users/LSmith/Documents/RStudio/bin/quarto/bin/tools/pandoc\" +RTS -K512m -RTS \"C:/Users/LSmith/Documents/Github/report.knit.md\" --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandoc14ec7f1f6453.html --lua-filter \"C:\\Users\\LSmith\\AppData\\Local\\Programs\\R\\R-4.2.1\\library\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\" --lua-filter \"C:\\Users\\LSmith\\AppData\\Local\\Programs\\R\\R-4.2.1\\library\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\" --self-contained --variable bs3=TRUE --section-divs --table-of-contents --toc-depth 6 --template \"C:\\Users\\LSmith\\AppData\\Local\\Programs\\R\\R-4.2.1\\library\\rmarkdown\\rmd\\h\\default.html\" --no-highlight --variable highlightjs=1 --variable theme=yeti --mathjax --variable \"mathjax-url=https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" --include-in-header \"C:\\Users\\LSmith\\AppData\\Local\\Temp\\Rtmpq4TZDj\\rmarkdown-str14ec7fb2d5a.html\" \n\n\n\nOutput created: report.html\n\n\n\n\n\n\n\n\n\n\nOne of the most useful visualizations it provides is the Correlation Heat Map. There are 9 relationships that indicate a correlation stronger than or equal to the absolute value of .15. The strongest being between Education and Age with a negative Pearson’s correlation of -.64. We can infer that this sample shows older individuals to have less education than their younger counterparts, which makes sense in a more recently industrialized country. The strongest positive correlation is between votesai and Taiwanese at .45. My theoretical reasoning supposes that those who are more closely tied to lands and culture in Taiwan are more likely to support the nationalist party over the other dominating party (Kuomintang or KMT).\n\n\n\nUsing the expss package is my easiest method of relabeling factor levels for visualization.\n\nlibrary(expss)\n\nWarning: package 'expss' was built under R version 4.2.2\n\n\nLoading required package: maditr\n\n\nWarning: package 'maditr' was built under R version 4.2.2\n\n\n\nTo get total summary skip 'by' argument: take_all(mtcars, mean)\n\n\n\nAttaching package: 'maditr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    copy, dcast, melt\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, coalesce, first, last\n\n\n\nAttaching package: 'expss'\n\n\nThe following objects are masked from 'package:data.table':\n\n    copy, like\n\n\nThe following object is masked from 'package:DataExplorer':\n\n    split_columns\n\n\nThe following objects are masked from 'package:dplyr':\n\n    compute, contains, na_if, recode, vars\n\n\nThe following objects are masked from 'package:haven':\n\n    is.labelled, read_spss\n\nChosenRows$Tondu <- factor(ChosenRows$Tondu, levels=c('1','2','3', '4', '5','6', '9'), labels=c('Unification now','Status quo, unif. in future','Status quo, decide later', 'Status quo forever', 'Status quo, indep. in future', 'Independence now', 'No response'))\n\n\n\n\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:expss':\n\n    vars\n\nggplot(ChosenRows, aes(Tondu)) + \ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nscale_y_continuous(labels=scales::percent) +\nylab(\"Support for Unification (%)\") + \nxlab(\"Stances on Unification\") +\ntheme_bw()\n\n\n\n\n\nlibrary(descr)\n\nWarning: package 'descr' was built under R version 4.2.2\n\nfreq(ChosenRows$Tondu)\n\n\n\n\nChosenRows$Tondu \n                             Frequency Percent\nUnification now                     27   1.598\nStatus quo, unif. in future        180  10.651\nStatus quo, decide later           546  32.308\nStatus quo forever                 328  19.408\nStatus quo, indep. in future       380  22.485\nIndependence now                   108   6.391\nNo response                        121   7.160\nTotal                             1690 100.000"
  },
  {
    "objectID": "Lab3.html",
    "href": "Lab3.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "Regression object\n\npetal_lm <- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "Haley Puddy\nHoda Osama\nLibby Smith\n\n\n\nThe purpose of this study is to reaffirm and discover what risk and protective factors increase or decrease the likelihood of victimization among a large sample of college students. There is a wealth of knowledge in this area showing that mental health problems and engagement in risky behaviors, like partying, sex, alcohol usage, and drug usage,significantly increase one’s likelihood of being victimized (Azimi & Daigle, 2021; Turanovic et al., 2018). Additionally, previous research has found social connectedness to be one of the strongest protective factors against victimization (McLoughlin et al., 2019; Kast et al., 2016). This research goes in line with social control and bond theories that argue that individuals are prevented from engaging in delinquency, and thus victimization, by having strong bonds to society, family, friends, and conventional activities (Hirschi, 1969). Literature is lacking, however, on protective factors related to overall health and propensity to make safer decisions. This study aims to contribute to the literature by reaffirming what is already clear about the risk and protective factors of victimization, as well as adding to the literature about additional protective factors that may be negatively associated with victimization. The results of this study have potential to build upon thewell-researched base on victimization and suggest new protective factors for other researchers to take a closer look at.\n\n\n\n\n\n1. What risk factors increase the likelihood of victimization for college students?\n2. What protective factors decrease the likelihood of victimization for college students?\n\n\n\n3. Is making certain healthier and safer decisions negatively correlated to victimization?\n4. Is engaging in specific risky behaviors positively correlated to victimization?\n5. Does the data confirm social control/bond theory in reducing incidences of victimization?\n\n\n\n\nOur analysis plan includes using secondary data collected from the largest known dataset on the health of college students calledThe American Health College Association-National College Health Assessment (ACHA-NCHA III). It is a nationally recognized research survey consisting of 52 items on college student’s health habits, behaviors, and perceptions. The data used for this project will be from the Spring of 2022, where 69,131 U.S. college students from across the country participated and completed the survey. Descriptive statistics using t-tests and chi-squares will be used as well as logistic regression models to measure what risk and protective factors have a positive or negative effect on victimization. Additionally, interaction and moderation effects will be used to determine if any combination of risk or protective factors exacerbate the effects on victimization.\n\n\n\nDV: Victimization (violent, sexual, IPV)–categorical\nIV(s): Risk & Protective factors for victimization–categorical & continuous\nRisk factors: Engagement in risky behaviors (sex, alcoholusage, partying, fighting, and drug usage)Mental health problems\nProtective factors: Overall health, Healthy decision making, Social connectedness, Sleep health, Safe driving\nControls: race/ethnicity, relationship status, gender, age"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "EPPS 6323: Lab03",
    "section": "",
    "text": "female, DPP, age, income, edu, Taiwanese, and Econ_worse, Tondu. Tondu is again our 7 level factor output variable that is a nominal categorical vector. First we prepare the data as we did the last assignment:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(haven)\nTEDS_2016 <- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nChosenRows <- TEDS_2016 %>% select(female, DPP, age, income, edu, Taiwanese, Tondu, Econ_worse)\n\nThus, I select a multinomial logistic regression to understand how the other variables like education or outlook on the economy may influence a voter’s decision on the question of Taiwans reunification. To do this, I use the nnet package.\n\nlibrary(nnet)\nmultinom_mod <- multinom(Tondu ~ female+ DPP+ age+ income+ edu+ Taiwanese+ Econ_worse, data = ChosenRows, model = T)\n\n# weights:  63 (48 variable)\ninitial  value 3269.129050 \niter  10 value 2805.464571\niter  20 value 2512.761717\niter  30 value 2422.517437\niter  40 value 2400.896134\niter  50 value 2396.364453\niter  60 value 2395.932333\niter  70 value 2395.903807\nfinal  value 2395.903672 \nconverged\n\nsummary(multinom_mod) #Logit Coefficients  \n\nCall:\nmultinom(formula = Tondu ~ female + DPP + age + income + edu + \n    Taiwanese + Econ_worse, data = ChosenRows, model = T)\n\nCoefficients:\n  (Intercept)      female      DPP          age       income        edu\n2  -0.2496151 -0.52913290 12.87587  0.009226620  0.092208263  0.4545365\n3   2.2364370 -0.12931722 13.31397 -0.030826924  0.078299839  0.4759185\n4   1.1228490  0.08600408 12.67246 -0.008274928  0.068445396  0.3467795\n5   0.2695572 -0.18626439 14.38817 -0.040016005  0.079054551  0.6154655\n6   0.2080110 -0.11208492 14.28383 -0.047874117 -0.002012347  0.1488133\n9   0.5228255  0.86755150 11.47414  0.011092331 -0.007219261 -0.1978480\n   Taiwanese  Econ_worse\n2 -0.5632312  0.21215947\n3  0.4827203  0.17011562\n4  0.7422269 -0.05123013\n5  1.9073122  0.12670491\n6  3.1218172  0.20219140\n9  0.5589865 -0.46961390\n\nStd. Errors:\n  (Intercept)    female       DPP        age     income       edu Taiwanese\n2    1.456217 0.4399858 0.1944427 0.01669182 0.08242167 0.2006279 0.4592553\n3    1.406182 0.4224477 0.1181860 0.01620944 0.07972263 0.1953428 0.4349737\n4    1.420168 0.4253504 0.1420111 0.01631119 0.08021841 0.1967471 0.4372073\n5    1.448421 0.4334210 0.1305801 0.01670160 0.08202136 0.2009154 0.4578560\n6    1.610807 0.4635765 0.1923037 0.01810068 0.08837476 0.2158939 0.6673997\n9    1.551476 0.4655179 0.2992296 0.01770726 0.08547755 0.2189487 0.4638317\n  Econ_worse\n2  0.4299031\n3  0.4137212\n4  0.4161689\n5  0.4259432\n6  0.4598972\n9  0.4435058\n\nResidual Deviance: 4791.807 \nAIC: 4887.807 \n\n\nAssuming model tests and validation affirm the output, we can write out the following research statements as a result: The multinomial logistic regression model showed that age is a significant predictor of Taiwanese voters’ preferences for reunification with China. Specifically, a one-unit increase in age is associated with a statistically significant decrease in the log odds of preferring independence now (β = -0.0479, SE = 0.0235, p < .05) and a statistically significant increase in the log odds of preferring status quo forever (β = -0.0083, SE = 0.0039, p < .05). These results suggest that older Taiwanese voters are more likely to prefer the status quo option, and less likely to support independence now, compared to younger voters.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nAnova(multinom_mod) #Significance testing with Liklihood Ratio\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: Tondu\n           LR Chisq Df Pr(>Chisq)    \nfemale       27.818  6  0.0001017 ***\nDPP         174.934  6  < 2.2e-16 ***\nage          73.842  6  6.644e-14 ***\nincome        8.456  6  0.2065438    \nedu          63.480  6  8.813e-12 ***\nTaiwanese   169.517  6  < 2.2e-16 ***\nEcon_worse    9.651  6  0.1401354    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of the chi-squared test indicated that the association between the seven-level outcome variable (representing Taiwanese voters’ preferences for reunification with China) and the predictors of age, sex, partisanship, residency, and education were all statistically significant at the p < .05 level. However, the predictors of income and economic outlook (econ_worse) did not achieve statistical significance in this analysis. These findings suggest that income and economic outlook may not play a significant role in shaping Taiwanese voters’ preferences for reunification with China, compared to the other factors included in the model. It’s important to note that the other predictors did demonstrate a significant association with the outcome variable, providing evidence for the influence of these variables on Taiwanese voters’ attitudes toward reunification.\nThis study examined the relationship between demographic and attitudinal factors, including age, sex, partisanship, income, education, residency, and economic outlook, and Taiwanese voters’ preferences on the issue of reunification with China. Results from the multinomial logistic regression indicate that several factors significantly influence these preferences. Specifically, respondents who identified as female were more likely to prefer status quo or independence, while those who identified with the Democratic Progressive Party (DPP) were more likely to prefer unification in the future or status quo with a decision later. Older age was associated with a preference for status quo, while higher income was associated with a preference for unification in the future or status quo with a decision later. Respondents with higher levels of education were more likely to prefer status quo or independence, while those who live in Taiwan were more likely to prefer status quo or independence in the future. Finally, respondents who held a more negative economic outlook were more likely to prefer independence now. These findings have implications for understanding the complex attitudes of Taiwanese voters towards reunification with China and may inform future efforts to address this issue.\n\n\n\nHad we not accounted for the data types of the variables, running a linear regression with factor or categorical variables in the dependent output would have caused some confusion. Since the dependent variable is a nominal unranked scale of options selected by the survey respondents, using a linear model or visualizing the data through continuous-friendly visualizations will result in some unhelpful funky graphs, as seen below. The below commands result in a regplot. A regression plot is particularly useful for identifying trends or patterns in the data, and for visualizing the strength and direction of the relationship between a continuous dependent variables and input variables. The plot includes a scatterplot of the data points, with the regression line overlaid on top. Below we listed the commands for the regplot using only the dependent variable Tondu and independent variables Age, Income, and Edu.\n\nregplot=function(x,y,...){ #Save the linear function in environment\n     fit=lm(y~x)\n     plot(x,y,...)\n    abline(fit,col=\"red\")\n}\nregplot(ChosenRows$age+ChosenRows$income+ChosenRows$edu, ChosenRows$Tondu, xlab=\"Age, Income, and Education\", ylab=\"Tondu\", col=\"blue\", pch=15)\n\n\n\n\nThe resulting graph is unreadable due to the specified model being unfit to properly analyse the data. To improve the model, we select a multinomial logistic regression. This also renders the need for normality assumption redundant as it uses Z scores instead of t-test statistic. However, model validation for multinomial logistic regression requires dealing meaningfully with multicollinearity and sample sizes."
  }
]